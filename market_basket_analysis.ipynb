{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jlo9IeujSdcm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from browsing.txt\n",
        "file_path = \"/content/browsing.txt\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    transactions = [set(line.strip().split()) for line in f]\n",
        "\n",
        "# Define minimum support threshold\n",
        "min_support = 100\n",
        "\n",
        "# Step 1: Count individual items\n",
        "item_counts = defaultdict(int)\n",
        "for transaction in transactions:\n",
        "    for item in transaction:\n",
        "        item_counts[item] += 1\n",
        "\n",
        "# Filter frequent items\n",
        "frequent_items = set()\n",
        "for item, count in item_counts.items():\n",
        "    if count >= min_support:\n",
        "        frequent_items.add(item)\n",
        "\n",
        "\n",
        "# Step 2: Generate frequent itemsets of size 2\n",
        "pair_counts = defaultdict(int)\n",
        "for transaction in transactions:\n",
        "    valid_items = transaction.intersection(frequent_items)\n",
        "    for pair in combinations(valid_items, 2):\n",
        "        pair_counts[frozenset(pair)] += 1\n",
        "\n",
        "# Filter frequent pairs\n",
        "frequent_pairs = {}\n",
        "for pair, count in pair_counts.items():\n",
        "    if count >= min_support:\n",
        "        frequent_pairs[pair] = count\n",
        "\n",
        "# Step 3: Generate frequent itemsets of size 3\n",
        "triple_counts = defaultdict(int)\n",
        "for transaction in transactions:\n",
        "    valid_items = transaction.intersection(frequent_items)\n",
        "    for triple in combinations(valid_items, 3):\n",
        "        if (frozenset([triple[0], triple[1]]) in frequent_pairs and\n",
        "            frozenset([triple[0], triple[2]]) in frequent_pairs and\n",
        "            frozenset([triple[1], triple[2]]) in frequent_pairs):\n",
        "            triple_counts[frozenset(triple)] += 1\n",
        "\n",
        "# Filter frequent triples\n",
        "frequent_triples = {}\n",
        "for triple, count in triple_counts.items():\n",
        "    if count >= min_support:\n",
        "        frequent_triples[triple] = count\n",
        "\n",
        "\n",
        "# Step 4: Compute Association Rules for Pairs (d)\n",
        "association_rules = []\n",
        "for pair, support_xy in frequent_pairs.items():\n",
        "    item_x, item_y = tuple(pair)\n",
        "    support_x = item_counts[item_x]\n",
        "    support_y = item_counts[item_y]\n",
        "\n",
        "    confidence_x_to_y = support_xy / support_x\n",
        "    confidence_y_to_x = support_xy / support_y\n",
        "\n",
        "    association_rules.append((item_x, item_y, confidence_x_to_y))\n",
        "    association_rules.append((item_y, item_x, confidence_y_to_x))\n",
        "\n",
        "# Sort rules for pairs by confidence (descending), breaking ties lexicographically\n",
        "association_rules_sorted = sorted(association_rules, key=lambda x: (-x[2], x[0], x[1]))\n",
        "\n",
        "# Step 5: Compute Association Rules for Triples (e)\n",
        "triple_rules = []\n",
        "for triple, support_xyz in frequent_triples.items():\n",
        "    x, y, z = sorted(triple)  # Ensure consistent ordering\n",
        "\n",
        "    # Compute confidence for each rule\n",
        "    support_xy = frequent_pairs.get(frozenset([x, y]), 0)\n",
        "    support_xz = frequent_pairs.get(frozenset([x, z]), 0)\n",
        "    support_yz = frequent_pairs.get(frozenset([y, z]), 0)\n",
        "\n",
        "    if support_xy > 0:\n",
        "        confidence_xy_to_z = support_xyz / support_xy\n",
        "        triple_rules.append(((x, y), z, confidence_xy_to_z))\n",
        "\n",
        "    if support_xz > 0:\n",
        "        confidence_xz_to_y = support_xyz / support_xz\n",
        "        triple_rules.append(((x, z), y, confidence_xz_to_y))\n",
        "\n",
        "    if support_yz > 0:\n",
        "        confidence_yz_to_x = support_xyz / support_yz\n",
        "        triple_rules.append(((y, z), x, confidence_yz_to_x))\n",
        "\n",
        "# Sort rules for triples by confidence (descending), breaking ties lexicographically\n",
        "triple_rules_sorted = sorted(triple_rules, key=lambda r: (-r[2], r[0][0], r[0][1]))\n",
        "\n",
        "# Convert association rules for pairs (d) to DataFrame\n",
        "association_rules_df = pd.DataFrame(association_rules_sorted[:5], columns=['LHS (X)', 'RHS (Y)', 'Confidence'])\n",
        "\n",
        "# Convert association rules for triples (e) to DataFrame\n",
        "triple_rules_df = pd.DataFrame(triple_rules_sorted[:5], columns=['LHS (X, Y)', 'RHS (Z)', 'Confidence'])\n",
        "\n",
        "# Display results\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTop 5 Association Rules for Pairs (d)\\n\", association_rules_df)\n",
        "print(\"\\nTop 5 Association Rules for Triples (e)\\n\", triple_rules_df)\n",
        "\n",
        "\n",
        "# Perform the first sanity check: Count the number of frequent items (L1)\n",
        "L1_count = len(frequent_items)\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\nSanity Check 1: Number of frequent items after the first pass (L1) = {L1_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRoNHA3ROIcx",
        "outputId": "01270671-25b4-4168-a958-8cbf6697b130"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Association Rules for Pairs (d)\n",
            "     LHS (X)   RHS (Y)  Confidence\n",
            "0  PFE66607  AOS75563    1.000000\n",
            "1  MOE13579  AOS75563    0.999176\n",
            "2  SOS23713  AOS75563    0.990654\n",
            "3  RDJ13710  AOS75563    0.990566\n",
            "4  FEZ23171  AOS75563    0.986726\n",
            "\n",
            "Top 5 Association Rules for Triples (e)\n",
            "              LHS (X, Y)   RHS (Z)  Confidence\n",
            "0  (DAI23334, ELE92920)  DAI62779         1.0\n",
            "1  (DAI31081, MOE13579)  AOS75563         1.0\n",
            "2  (DAI55911, MOE13579)  AOS75563         1.0\n",
            "3  (DAI62779, FEZ23171)  AOS75563         1.0\n",
            "4  (DAI75645, MOE13579)  AOS75563         1.0\n",
            "\n",
            "Sanity Check 1: Number of frequent items after the first pass (L1) = 647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q6msxECtPXXL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}